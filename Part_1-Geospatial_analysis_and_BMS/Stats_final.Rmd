---
title: "Biogeographical patterns on quercus and poaceae species across Spain by means of LDA"
author: "Eric Bataller"
output: html_notebook
---

```{r}
library(sp)
library(raster)
library(rgbif)
library(protolite)
library(tidyverse)
library(quanteda) 
library(quanteda.textplots)
library(topicmodels)
library(LDAvis) 
library(lda)

library(glue)
library(dplyr)
library(ggplot2)
library(readxl)
library(stringr)
library(colorspace)
library(sf)
set.seed(5528)
```

# 1. Load the shape files of municipalities in spain:

```{r}
base_dir <- "C:/Users/ericb/Desktop/Stats_final/lineas_limite"

peninbal_shp_filename <- glue("{base_dir}/SHP_ETRS89/recintos_municipales_inspire_peninbal_etrs89/recintos_municipales_inspire_peninbal_etrs89.shp")
canarias_shp_filename <- glue("{base_dir}/SHP_WGS84/recintos_municipales_inspire_canarias_wgs84/recintos_municipales_inspire_canarias_wgs84.shp")

peninbal_shp <- read_sf(peninbal_shp_filename, quiet = TRUE) #This returns an sf-tibble type data (contains polygons)
canarias_shp <- read_sf(canarias_shp_filename, quiet = TRUE)
```

A sf-tibble is similar to a normal tibble, with an entry for each of the territorial units contained in the file (in this case, a municipality).
The different columns correspond to properties associated with each province, like, for example, its name or the id code. The last column of a sf-tibble is special, since it must be present in all data files of these type: the so-called geometry, which contains the list of points that conform all the polygons of the province.

```{r}
peninbal_shp %>% head() %>% print(width = 120)
```

A sf-tibble is rather easy to plot. The ggplot2 library contains a specific geom_ for this kind of data, therefore drawing it is just a matter of making the following call:

```{r}
peninbal_shp %>% 
    ggplot() +
    geom_sf() +
    theme_bw()
```

Plotting the map is very slow, specially the Peninsula. This is due to the fact that the polygons of the municipalities are extremely detailed and contain a lot of points.
In order to solve this issue, we can simplify the geometry. This step can cause some polygons to be rendered not as accurately as in the original map, but in practice, given the level of detail we are interested in, we can safely ignore this. When simplifying we must specify the tolerance in decimal degrees. In our case, we have determined that a tolerance value of 1e-03 produces sufficently detailed polygons while at the same time greatly speeding the rendering process.

```{r}
peninbal_simpl_shp <- 
    peninbal_shp %>% 
    st_simplify(dTolerance = 1e-04)

canarias_simpl_shp <- 
    canarias_shp %>% 
    st_simplify(dTolerance = 1e-04)

peninbal_simpl_shp %>% 
    ggplot() +
    geom_sf() +
    theme_bw()
```

The next thing we would like to do is to combine both sf-tibbles, so that the Peninsula, Balearics, and the Canary Islands all show up in a single map. Again, this is a simple task within the sf package: just use the rbind function to append the rows of one dataset to the other. However, we must first adjust the coordinate reference system, or CRS. The polygons of the Canary Islands have coordinates with respect to a CRS that is different from the one used by the coordinates of the Peninsula and Balearics. If we want to combine them, we must first transform the coordinates of one of them to the CRS of the other.

```{r}
# Transform Canary Islands to use the coordinate reference system (CRS) of the peninsula-balearics.
# This is a necessary step before unioning because peninsula-balearics and canarias have
# different CRS.
canarias_transformed_shp <- 
    canarias_simpl_shp %>% 
    st_transform(st_crs(peninbal_simpl_shp)) 
    
# Union peninsula-balearics + canarias
espana_shp <- rbind(peninbal_simpl_shp, canarias_transformed_shp)
```

Once combined, the final result is the following:

```{r}
espana_shp %>% 
    ggplot() +
    geom_sf() +
    theme_bw()
```

# 2.Loading the occurrance data:

```{r}
fields <- c("key",
            "decimalLatitude",
            "decimalLongitude",
            "family",
            "species")



quercus <- occ_search(country = "ES",
                      datasetKey = "fab4c599-802a-4bfc-8a59-fc7515001bfa",
                      familyKey = 4689,
                      fields = fields,
                      limit  = 80000)
quercus_data <- quercus$data


pastos <- occ_search(country = "ES",
                     datasetKey = "4cf3eec1-b902-40c9-b15b-05c5fe5928b6",
                     familyKey = 3073,
                     fields = fields,
                     limit = 100000)

pastos_data <- pastos$data
```

Put both quercus and pastos dataframes together:

```{r}
occurrances_df <- bind_rows(quercus_data, pastos_data)
```

```{r}

pnts <- occurrances_df[c("decimalLongitude","decimalLatitude")] #Dataframe just with the coordinates of occ
# create a points collection
pnts_sf <- do.call("st_sfc",c(lapply(1:nrow(pnts), 
function(i) {st_point(as.numeric(pnts[i, ]))}), list("crs" = 4326))) 

pnts_trans <- st_transform(pnts_sf, 2163) # apply transformation to pnts sf
espana_shp_trans <- st_transform(espana_shp, 2163)      # apply transformation to polygons sf

# intersect and extract municipality name
occurrances_df$region <- apply(st_intersects(espana_shp_trans, pnts_trans, sparse = FALSE), 2, 
               function(col) { 
                  espana_shp_trans[which(col),]$NATCODE
               })

#The above line returns a column of type list of char objects, let's make it a string column
occurrances_df <- occurrances_df %>% 
  mutate(region = sapply(region, toString))
```

# 3. Corpus creation and layout preparation

We now create the "documents" that we will use in our LDA. Each municipality makes a document with a set of "words" (each occurrence is a word - the name of the species observed).

```{r}
documents_df <- occurrances_df %>% 
  group_by(region, species) %>%
  add_count(name = "Count") %>%
  distinct(species, .keep_all = TRUE) 

documents_df$species <- lapply(documents_df$species, gsub, pattern = " ", replacement = "-", fixed = TRUE)#Replace spaces in "species" column for "-".

documents_df$word <- paste(documents_df$family,documents_df$species,sep= "-") #Create a "word" column that concatenates family and species name

documents_df$speech <- apply(documents_df, 1, function(x) paste(replicate(x$Count, x$word), collapse = " ")) #Create the speech column repeating the word (species) by count (the amount of times it was observed). Note that this is not yet the final speech, we have to group them by region to have all words in that document (region).

#Group by region and get the full speech (the concatenation of species seen in that region)
documents_df <- documents_df %>%
  group_by(region) %>%
  summarise(full_speech = paste(speech, collapse = " "))

documents_df = documents_df[-1,] #We drop the first row as it is where all the occurrences that couldn't be assigned to a region went to
```

```{r}
head(documents_df)
```

Let's inspect the lenght of each document:

```{r}
documents_df$ntoken <- ntoken(documents_df$full_speech)
hist(documents_df$ntoken, main = "Histogram of speech length", xlim = c(0, 500), breaks = 1000)
```

```{r}
#Create corpus:
speeches_Corpus <- corpus(documents_df, docid_field = 'region', text_field = 'full_speech')
summary(speeches_Corpus, 5)
```

We create the corpus and retain only documents with a length larger than 10.

```{r}
#Filtering doc lenght:
ntokens_corpus <- ntoken(speeches_Corpus)
docs_length15 <- names(ntokens_corpus[(ntokens_corpus>=10)])
speeches_Corpus <- speeches_Corpus[names(speeches_Corpus) %in% docs_length15]

#Print summary:
summary(speeches_Corpus, n=4) 
head(speeches_Corpus)
```

# Turn this corpus into a document-feature matrix.

```{r}
custom_list_remove <- c("fagaceae-quercus-rotundifolia") #List of words to be removed

speechDFM_filtered <- tokens(speeches_Corpus) %>%
                      tokens_remove(custom_list_remove) %>%
                      dfm() %>% 
                      dfm_tolower() %>%
                      dfm_trim(min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")
speechDFM_filtered <- dfm_subset(speechDFM_filtered, ntoken(speechDFM_filtered) > 0) #Rmove any empty document
speechDFM_filtered
```
```{r, warning=FALSE}
textplot_wordcloud(speechDFM_filtered, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```


With the function `topfeatures` we can extract the most frequent terms in our document$\times$term frequency matrix.

```{r}
topfeatures(speechDFM_filtered, 20)
```

## TF-IDF

Latent Dirichlet Allocation takes as input a document$\times$term frequency matrix. However, another popular representation of a corpus is a TF-IDF matrix which has the same structure as a document$\times$term frequency matrix but the term frequencies (TF) are weighted by the inverse document frequency (IDF). The IDF is the inverse of the number of documents in which the term occurs, log scaled. The objective is to downscale the matrix entries of frequent terms in the corpus and upscale the matrix entries of terms specific to a group of documents.

```{r}
speechDFM_filtered_tfidf <- tokens(speeches_Corpus) %>% 
                      tokens_remove(custom_list_remove) %>%
                      dfm() %>% 
                      dfm_tolower() %>%
                      dfm_trim(min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop") %>%
                      dfm_tfidf()
speechDFM_filtered_tfidf
```

```{r, warning=FALSE}
textplot_wordcloud(speechDFM_filtered_tfidf, random_order = FALSE, rotation = 0.25, 
    color = RColorBrewer::brewer.pal(8, "Dark2"))
```

# Latent Dirichlet Allocation

Now that have our corpus representation ready we can run our topic models. In `R` there are several packages implementing topic models. In this seminar will we use the `lda` package that implements a collapsed Gibbs sampling for Latent Dirichlet Allocation (LDA) and supervised LDA. Another package is `topicmodels` that implements Latent Dirichlett Allocation and Correlated Topic Models estimated with a variational expectation-maximization algorithm or Gibbs sampling.

We convert our corpus to an object that has the format required by package `lda`. We use the function `convert` from `quanteda` to do that. 

```{r}
# convert to lda format
library(tm)
biodiversity_lda <- convert(speechDFM_filtered, to = "lda")
```

We estimate our LDA model. We need to set the number of topics $K$, the number of iterations $G$, and Dirichlet priors hyperparameters.

```{r}
# MCMC and model tuning parameters:
K <- 8 # number of topics
G <- 2000 # number of iterations
eta <- 1/K # Dirichlet hyperparamater for topic multinomials
alpha <- 1/K # Dirichlet hyperparameter for topic proportions

# fit the model
t1 <- Sys.time()
lda_fitted <- lda.collapsed.gibbs.sampler(documents = biodiversity_lda$documents, K = K, 
                                   vocab = biodiversity_lda$vocab, 
                                   num.iterations = G, alpha = alpha, 
                                   eta = eta, initial = NULL, burnin = 0,
                                   compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
```

We can use the function `top.topic.words` to extract the top words by topic, that is the most probable words in each topic.

```{r}
top.topic.words(lda_fitted$topics,num.words=20)
```

We can use the function `top.topic.documents` to extract the top documents by topic, that is the documents most associated with each topic.

```{r}
top_docs <- top.topic.documents(lda_fitted$document_sums, num.documents = 5)
```


Next, we obtain the DÃ—K community proportions matrix where we get the proportion of each topic assigned to each sample unit.
```{r}
thetas <- as.data.frame(t(lda_fitted$document_sums), row.names= speechDFM_filtered@docvars[["docname_"]])
thetas <- thetas/rowSums(thetas) 
thetas$NATCODE <- row.names(thetas)
```

# Mapping the biogeographical pattern of each topic

Shifting Canary Islands coordinates for visualisation:

```{r}
crs <- st_crs(peninbal_simpl_shp) # extract CRS from Peninsula-Balearics (we'll need it later)

canarias_geom <- 
    canarias_simpl_shp %>% 
    st_transform(crs) %>%  # transform Canary Islands to use the CRS of Peninsula-Balearics
    st_geometry()  # extract geometry (polygons)
```

Once the geometry is extracted, we can shift all the points of the polygons by adding a 2-component vector with (lat, long) coordinates, as follows:

```{r}
canarias_shifted_geom <- canarias_geom + c(5, 7)  # shift Canary Islands polygons closer to peninsula
```

Lastly, we reincorporate this new shifted geometry to the Canary Islands data, taking care of reestablishing the CRS again, because the act of assigning a geometry to an sf-tibble resets its CRS:

```{r}
canarias_shp_visual <- 
    canarias_simpl_shp %>% 
    st_set_geometry(canarias_shifted_geom) %>%  # set new shifted geometry to Canary Islands
    st_set_crs(crs)  # restore CRS (setting geometry resets CRS to NULL)
    
espana_shp_visual <- rbind(peninbal_simpl_shp, canarias_shp_visual)
```

Finally, create a function to remove the coordinates in the axis: (we'll use it later)

```{r}
theme_custom_map <- function(base_size = 11,
                             base_family = "",
                             base_line_size = base_size / 22,
                             base_rect_size = base_size / 22) {
    theme_bw(base_size = base_size, 
             base_family = base_family,
             base_line_size = base_line_size) %+replace%
        theme(
            axis.title = element_blank(), 
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            complete = TRUE
        )
}
```

Merge the proportions of each topic the polygon in the map it belongs to:

```{r}
drawing_data <- 
    espana_shp_visual %>% 
    left_join(thetas, by = 'NATCODE')
```

```{r}
head(drawing_data)
```

```{r}
drawing_data %>% 
    ggplot() +
    aes(fill = V4) +
    geom_sf(size = 0.001) +
    scale_fill_gradientn(colours = rev(grDevices::heat.colors(10)), name = NULL) +
    theme_custom_map() + 
    ggtitle("Topic 6 proportion")
```

Let's plot all of them:

```{r}
for (i in tail(colnames(drawing_data),K)){
   nam <- paste("p_", i, sep = "")
   p <- drawing_data %>% 
     ggplot() +
      aes_string(fill = i) +
      geom_sf(size = 0.001) +
      scale_fill_gradientn(colours = rev(grDevices::heat.colors(10)), name = NULL, guide="none") +
      theme_custom_map() +
      ggtitle(paste("Topic", substring(i, 2), "proportion", sep = " ")) +
      theme(plot.title = element_text(size = 10, face = "bold"))

   assign(nam, p)
}
```

```{r, fig.width=10,fig.height=15}
library(cowplot)
plot_grid(p_V1, p_V2, p_V3, p_V4,p_V5,p_V6,p_V7,p_V8, nrow = 4, ncol=2, scale=1)
```



